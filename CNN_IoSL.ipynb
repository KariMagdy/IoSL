{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for IoSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes code for training a CNN to classify songs according to their intelligibility to one of three classes (High, Moderate, Low).\n",
    "\n",
    "The CNN is trained on the melspectrogram of the whole song. Songs are padded to a length of (x) where x is the longest file in the dataset.\n",
    "\n",
    "Requirements: \n",
    "- Keras\n",
    "- Librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import fnmatch\n",
    "import os\n",
    "from os.path import isfile\n",
    "import csv\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing parameters. \n",
    "\n",
    "__Note:__ Change directories accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 3\n",
    "epochs = 10\n",
    "data_augmentation = False\n",
    "num_predictions = 3\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_IoSL_trained_model.h5'\n",
    "# Directory of the dataset\n",
    "datasetDir = \"/Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/\"\n",
    "# Directory of the saved melspectrogram\n",
    "outpath = \"/Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Mels/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code organizes the dataset in three seperate folders depending on their class. It also computes the melspectrogram for each file and save it similarily into seperate folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and saving mels\n",
    "# Copying files to the three classes folders\n",
    "def load_labels(labelsDir = '/Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/labels_genres.csv'):\n",
    "    name,label,genre = [],[],[]\n",
    "    with open(labelsDir) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            name.append(row[0])\n",
    "            label.append(row[1])\n",
    "            genre.append(row[2])\n",
    "    return name,label,genre\n",
    "\n",
    "def copy_files_to_directoriesClasses(directory = \"/Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/FullDataset/\"\n",
    "):\n",
    "    name,label,genre = load_labels()\n",
    "    files = fnmatch.filter(os.listdir(directory), '*.wav')\n",
    "    for filename in files:\n",
    "        idx = name.index(filename)\n",
    "        score = float(label[idx])\n",
    "        if score < 0.33:\n",
    "            copyfile(directory + filename, datasetDir + \"Low/\" + '[' + str(\"{0:.2f}\".format(score)) + ']' + filename)\n",
    "        elif score < 0.66:\n",
    "            copyfile(directory + filename, datasetDir + \"Moderate/\" + '[' + str(\"{0:.2f}\".format(score)) + ']' + filename)\n",
    "        else:\n",
    "            copyfile(directory + filename, datasetDir + \"High/\" + '[' + str(\"{0:.2f}\".format(score)) + ']' + filename)\n",
    "\n",
    "def get_class_names(path=datasetDir):  # class names are subdirectory names in Samples/ directory\n",
    "    class_names = os.listdir(path)\n",
    "    return class_names\n",
    "\n",
    "def preprocess_dataset(inpath=datasetDir, outpath=outpath):\n",
    "    class_names = get_class_names(path=inpath)   # get the names of the subdirectories\n",
    "    nb_classes = len(class_names)\n",
    "    print(\"class_names = \",class_names)\n",
    "    for idx, classname in enumerate(class_names):   # go through the subdirs\n",
    "\n",
    "        if not os.path.exists(outpath+classname):\n",
    "            os.mkdir( outpath+classname );   # make a new subdirectory for preproc class\n",
    "\n",
    "        class_files = files = fnmatch.filter(os.listdir(inpath+classname), '*.wav')\n",
    "        n_files = len(class_files)\n",
    "        n_load = n_files\n",
    "        print(' class name = {:14s} - {:3d}'.format(classname,idx),\n",
    "            \", \",n_files,\" files in this class\",sep=\"\")\n",
    "\n",
    "        printevery = 20\n",
    "        for idx2, infilename in enumerate(class_files):\n",
    "            audio_path = inpath + classname + '/' + infilename\n",
    "            if (0 == idx2 % printevery):\n",
    "                print('\\r Loading class: {:14s} ({:2d} of {:2d} classes)'.format(classname,idx+1,nb_classes),\n",
    "                       \", file \",idx2+1,\" of \",n_load,\": \",audio_path,sep=\"\")\n",
    "            #start = timer()\n",
    "            aud, sr = librosa.load(audio_path, sr=None)\n",
    "            \"\"\"\n",
    "            Padding Audio to max size [705601] (Calculated seperatly on the longest file in this dataset, around 16 Sec)\n",
    "            \"\"\"\n",
    "            paddedAud = np.zeros(705601)\n",
    "            paddedAud[0:len(aud),] = aud\n",
    "            melgram = librosa.amplitude_to_db(librosa.feature.melspectrogram(paddedAud, sr=sr, n_mels=96))[np.newaxis,np.newaxis,:,:]\n",
    "            outfile = outpath + classname + '/' + infilename+'.npy'\n",
    "            np.save(outfile,melgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the melspectrogram files and parition it to train/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files =  215\n"
     ]
    }
   ],
   "source": [
    "def get_class_names(path=\"Preproc/\"):  # class names are subdirectory names in Preproc/ directory\n",
    "    class_names = os.listdir(path)\n",
    "    return class_names\n",
    "\n",
    "def get_total_files(path=\"Preproc/\",train_percentage=0.8): \n",
    "    sum_total = 0\n",
    "    sum_train = 0\n",
    "    sum_test = 0\n",
    "    subdirs = os.listdir(path)\n",
    "    for subdir in subdirs:\n",
    "        files = os.listdir(path+subdir)\n",
    "        n_files = len(files)\n",
    "        sum_total += n_files\n",
    "        n_train = int(train_percentage*n_files)\n",
    "        n_test = n_files - n_train\n",
    "        sum_train += n_train\n",
    "        sum_test += n_test\n",
    "    return sum_total, sum_train, sum_test\n",
    "\n",
    "def get_sample_dimensions(path='Preproc/'):\n",
    "    classname = os.listdir(path)[0]\n",
    "    files = os.listdir(path+classname)\n",
    "    infilename = files[0]\n",
    "    audio_path = path + classname + '/' + infilename\n",
    "    melgram = np.load(audio_path)\n",
    "    print(\"   get_sample_dimensions: melgram.shape = \",melgram.shape)\n",
    "    return melgram.shape\n",
    "\n",
    "def encode_class(class_name, class_names):  # makes a \"one-hot\" vector for each class name called\n",
    "    try:\n",
    "        idx = class_names.index(class_name)\n",
    "        vec = np.zeros(len(class_names))\n",
    "        vec[idx] = 1\n",
    "        return vec\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def shuffle_XY_paths(X,Y,paths):   # generates a randomized order, keeping X&Y(&paths) together\n",
    "    assert (X.shape[0] == Y.shape[0] )\n",
    "    idx = np.array(range(Y.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    newX = np.copy(X)\n",
    "    newY = np.copy(Y)\n",
    "    newpaths = paths\n",
    "    for i in range(len(idx)):\n",
    "        newX[i] = X[idx[i],:,:]\n",
    "        newY[i] = Y[idx[i],:]\n",
    "        newpaths[i] = paths[idx[i]]\n",
    "    return newX, newY, newpaths\n",
    "    \n",
    "def get_total_files(path=outpath,train_percentage=0.8): \n",
    "    sum_total = 0\n",
    "    sum_train = 0\n",
    "    sum_test = 0\n",
    "    subdirs = os.listdir(path)\n",
    "    for subdir in subdirs:\n",
    "        files = os.listdir(path+subdir)\n",
    "        n_files = len(files)\n",
    "        sum_total += n_files\n",
    "        n_train = int(train_percentage*n_files)\n",
    "        n_test = n_files - n_train\n",
    "        sum_train += n_train\n",
    "        sum_test += n_test\n",
    "    return sum_total, sum_train, sum_test\n",
    "\n",
    "total_files, total_train, total_test = get_total_files(path=outpath, train_percentage=0.8)\n",
    "print(\"total files = \",total_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all previous functions to load dataset from hardrive to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(train_percentage=0.8, preproc=False):\n",
    "    if (preproc):\n",
    "        path = outpath\n",
    "    else:\n",
    "        path = datasetDir\n",
    "        \n",
    "    class_names = get_class_names(path=path)\n",
    "    print(\"class_names = \",class_names)\n",
    "\n",
    "    total_files, total_train, total_test = get_total_files(path=path, train_percentage=train_percentage)\n",
    "    print(\"total files = \",total_files)\n",
    "\n",
    "    nb_classes = len(class_names)\n",
    "\n",
    "    # pre-allocate memory for speed (old method used np.concatenate, slow)\n",
    "    mel_dims = get_sample_dimensions(path=path)  # Find out the 'shape' of each data file\n",
    "    X_train = np.zeros((total_train, mel_dims[1], mel_dims[2], mel_dims[3]))   \n",
    "    Y_train = np.zeros((total_train, nb_classes))  \n",
    "    X_test = np.zeros((total_test, mel_dims[1], mel_dims[2], mel_dims[3]))  \n",
    "    Y_test = np.zeros((total_test, nb_classes))  \n",
    "    paths_train = []\n",
    "    paths_test = []\n",
    "\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    for idx, classname in enumerate(class_names):\n",
    "        this_Y = np.array(encode_class(classname,class_names) )\n",
    "        this_Y = this_Y[np.newaxis,:]\n",
    "        class_files = os.listdir(path+classname)\n",
    "        n_files = len(class_files)\n",
    "        n_load =  n_files\n",
    "        n_train = int(train_percentage * n_load)\n",
    "        printevery = 100\n",
    "        print(\"\")\n",
    "        for idx2, infilename in enumerate(class_files[0:n_load]):          \n",
    "            audio_path = path + classname + '/' + infilename\n",
    "            if (0 == idx2 % printevery):\n",
    "                print('\\r Loading class: {:14s} ({:2d} of {:2d} classes)'.format(classname,idx+1,nb_classes),\n",
    "                       \", file \",idx2+1,\" of \",n_load,\": \",audio_path,sep=\"\")\n",
    "            #start = timer()\n",
    "            if (preproc):\n",
    "              melgram = np.load(audio_path)\n",
    "              sr = 44100\n",
    "            else:\n",
    "              aud, sr = librosa.load(audio_path, mono=mono,sr=None)\n",
    "              melgram = librosa.logamplitude(librosa.feature.melspectrogram(aud, sr=sr, n_mels=96),ref_power=1.0)[np.newaxis,np.newaxis,:,:]\n",
    "\n",
    "            melgram = melgram[:,:,:,0:mel_dims[3]]   # just in case files are differnt sizes: clip to first file size\n",
    "       \n",
    "            #end = timer()\n",
    "            #print(\"time = \",end - start) \n",
    "            if (idx2 < n_train):\n",
    "                # concatenate is SLOW for big datasets; use pre-allocated instead\n",
    "                #X_train = np.concatenate((X_train, melgram), axis=0)  \n",
    "                #Y_train = np.concatenate((Y_train, this_Y), axis=0)\n",
    "                X_train[train_count,:,:] = melgram\n",
    "                Y_train[train_count,:] = this_Y\n",
    "                paths_train.append(audio_path)     # list-appending is still fast. (??)\n",
    "                train_count += 1\n",
    "            else:\n",
    "                X_test[test_count,:,:] = melgram\n",
    "                Y_test[test_count,:] = this_Y\n",
    "                #X_test = np.concatenate((X_test, melgram), axis=0)\n",
    "                #Y_test = np.concatenate((Y_test, this_Y), axis=0)\n",
    "                paths_test.append(audio_path)\n",
    "                test_count += 1\n",
    "        print(\"\")\n",
    "\n",
    "    print(\"Shuffling order of data...\")\n",
    "    X_train, Y_train, paths_train = shuffle_XY_paths(X_train, Y_train, paths_train)\n",
    "    X_test, Y_test, paths_test = shuffle_XY_paths(X_test, Y_test, paths_test)\n",
    "\n",
    "    return X_train, Y_train, paths_train, X_test, Y_test, paths_test, class_names, sr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X,Y,nb_classes):\n",
    "    nb_filters = 32  # number of convolutional filters to use\n",
    "    pool_size = (2, 2)  # size of pooling area for max pooling\n",
    "    kernel_size = (3, 3)  # convolution kernel size\n",
    "    nb_layers = 2\n",
    "    input_shape = (1, X.shape[2], X.shape[3])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nb_filters, kernel_size, strides=(1, 1),\n",
    "                        padding='valid', input_shape=input_shape,data_format=\"channels_first\"))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    for layer in range(nb_layers-1):\n",
    "        model.add(Conv2D(nb_filters, kernel_size, strides=(1, 1),data_format=\"channels_first\"))\n",
    "        model.add(BatchNormalization(axis=1))\n",
    "        model.add(ELU(alpha=1.0))  \n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(24))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_names =  ['High', 'Low', 'Moderate']\n",
      " class name = High           -   0, 92 files in this class\n",
      " Loading class: High           ( 1 of  3 classes), file 1 of 92: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/High/[0.67]excerpt1_Blessed_By_A_Broken_Heart_Show_Me_What_You_Got.wav\n",
      " Loading class: High           ( 1 of  3 classes), file 21 of 92: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/High/[0.74]excerpt1_Bobby Darin_Splish Splash.wav\n",
      " Loading class: High           ( 1 of  3 classes), file 41 of 92: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/High/[0.79]excerpt1_Ian Sylvia_Tomorrow Is A Long Time.wav\n",
      " Loading class: High           ( 1 of  3 classes), file 61 of 92: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/High/[0.87]excerpt1_Kelly Osbourne_Right Here.wav\n",
      " Loading class: High           ( 1 of  3 classes), file 81 of 92: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/High/[0.95]excerpt2_Bruce Willis_Pep Talk.wav\n",
      " class name = Low            -   1, 45 files in this class\n",
      " Loading class: Low            ( 2 of  3 classes), file 1 of 45: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Low/[0.00]excerpt1_3OH!3 Black Hole Omens.wav\n",
      " Loading class: Low            ( 2 of  3 classes), file 21 of 45: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Low/[0.07]excerpt2_ISetMyFriendsOnFire_ReesesPieces.wav\n",
      " Loading class: Low            ( 2 of  3 classes), file 41 of 45: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Low/[0.25]excerpt2_He Shall Feed his Flock.wav\n",
      " class name = Moderate       -   2, 77 files in this class\n",
      " Loading class: Moderate       ( 3 of  3 classes), file 1 of 77: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Moderate/0.37]excerpt2_AnyGivenDay_Endurance.wav\n",
      " Loading class: Moderate       ( 3 of  3 classes), file 21 of 77: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Moderate/[0.37]excerpt2_Steve_Aoki_Lit.wav\n",
      " Loading class: Moderate       ( 3 of  3 classes), file 41 of 77: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Moderate/[0.52]excerpt2_Marvin Gaye_The Masquerade Is Over.wav\n",
      " Loading class: Moderate       ( 3 of  3 classes), file 61 of 77: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Dataset_Labels/Moderate/[0.61]excerpt1_SteelPulse_TaxiDriver.wav\n",
      "class_names =  ['High', 'Low', 'Moderate']\n",
      "total files =  214\n",
      "   get_sample_dimensions: melgram.shape =  (1, 1, 96, 1379)\n",
      "\n",
      " Loading class: High           ( 1 of  3 classes), file 1 of 92: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Mels/High/[0.67]excerpt1_Blessed_By_A_Broken_Heart_Show_Me_What_You_Got.wav.npy\n",
      "\n",
      "\n",
      " Loading class: Low            ( 2 of  3 classes), file 1 of 45: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Mels/Low/[0.00]excerpt1_3OH!3 Black Hole Omens.wav.npy\n",
      "\n",
      "\n",
      " Loading class: Moderate       ( 3 of  3 classes), file 1 of 77: /Users/KarimM/GoogleDrive/PhD/Research/IoSLDataset/Mels/Moderate/0.37]excerpt2_AnyGivenDay_Endurance.wav.npy\n",
      "\n",
      "Shuffling order of data...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 94, 1377)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 94, 1377)      128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 94, 1377)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 92, 1375)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 92, 1375)      128       \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 32, 92, 1375)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 46, 1375)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 46, 1375)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1012000)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                24288024  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 75        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 24,297,923\n",
      "Trainable params: 24,297,795\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "preprocess_dataset() #==> run to get mels\n",
    "X_train, Y_train, paths_train, X_test, Y_test, paths_test, class_names, sr = build_datasets(preproc=True)\n",
    "\n",
    "# make the model\n",
    "model = build_model(X_train,Y_train, nb_classes=len(class_names))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adadelta',\n",
    "          metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from scratch (no checkpoint)\n",
      "Train on 170 samples, validate on 44 samples\n",
      "Epoch 1/5\n",
      "170/170 [==============================] - 547s 3s/step - loss: 3.3750 - acc: 0.3353 - val_loss: 8.7292 - val_acc: 0.4318\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8.72919, saving model to weights.hdf5\n",
      "Epoch 2/5\n",
      "170/170 [==============================] - 509s 3s/step - loss: 8.5742 - acc: 0.3706 - val_loss: 8.3979 - val_acc: 0.4091\n",
      "\n",
      "Epoch 00002: val_loss improved from 8.72919 to 8.39795, saving model to weights.hdf5\n",
      "Epoch 3/5\n",
      "170/170 [==============================] - 516s 3s/step - loss: 8.8062 - acc: 0.4059 - val_loss: 10.0938 - val_acc: 0.3636\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 8.39795\n",
      "Epoch 4/5\n",
      "170/170 [==============================] - 523s 3s/step - loss: 8.7195 - acc: 0.4000 - val_loss: 9.8906 - val_acc: 0.3864\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 8.39795\n",
      "Epoch 5/5\n",
      "170/170 [==============================] - 586s 3s/step - loss: 9.3417 - acc: 0.4000 - val_loss: 9.8906 - val_acc: 0.3864\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 8.39795\n",
      "Test score: 9.89064953543923\n",
      "Test accuracy: 0.38636363365433435\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights using checkpoint if it exists. (Checkpointing requires h5py)\n",
    "load_checkpoint = False\n",
    "checkpoint_filepath = 'weights.hdf5'\n",
    "if (load_checkpoint):\n",
    "    print(\"Looking for previous weights...\")\n",
    "    if ( isfile(checkpoint_filepath) ):\n",
    "        print ('Checkpoint file detected. Loading weights.')\n",
    "        model.load_weights(checkpoint_filepath)\n",
    "    else:\n",
    "        print ('No checkpoint file detected.  Starting from scratch.')\n",
    "else:\n",
    "    print('Starting from scratch (no checkpoint)')\n",
    "checkpointer = ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "# train and score the model\n",
    "batch_size = 128\n",
    "nb_epoch = 5\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch,\n",
    "      verbose=1, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
